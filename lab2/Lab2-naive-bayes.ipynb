{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.1"},"colab":{"name":"Lab2-naive-bayes.ipynb","provenance":[]}},"cells":[{"cell_type":"markdown","metadata":{"id":"UisOuoIAXgT6","colab_type":"text"},"source":["# Practical Machine Learning\n","# Lab 2"]},{"cell_type":"markdown","metadata":{"id":"YEn9Vd26XgT8","colab_type":"text"},"source":["## Naive Bayes\n","\n","We are going to classify the MNIST data using the Naive Bayes classifier using the **scikit-learn** library."]},{"cell_type":"markdown","metadata":{"id":"bTjAqTKlXgT8","colab_type":"text"},"source":["## Bayes' theorem"]},{"cell_type":"markdown","metadata":{"collapsed":true,"id":"ILstAW8VXgT9","colab_type":"text"},"source":["![bayes_rule.png](attachment:bayes_rule.png) "]},{"cell_type":"markdown","metadata":{"id":"WuBdJ5xbXgT-","colab_type":"text"},"source":["## Naive Bayes\n","\n","Naives Bayes method is a supervised learning algorithm based on applying Bayes' theorem with the '*naive*' assumption of conditional independence between every pair of features given the value of the class variable.\n","\n","Let *X* be a feature vector $X=\\{x_1, x_2,..., x_n\\}$ and $y_k$ be a class variable, the predicted label ($y_{hat}$) is:\n","$$y_{hat} = argmax_{y_k} P(y_k) \\prod_{i=1}^{i=n}P(x_i | y_k) $$\n","where $P(y_k)$ is the likelihood of class $y_k$ and $P(x_i | y_k)$ is the likelihood of feature $x_i$ in class $y_k$."]},{"cell_type":"markdown","metadata":{"id":"fQfa0JDCXgT_","colab_type":"text"},"source":["### Gaussian Naive Bayes\n","\n","The likelihood of the features is assumed to be Gaussian:\n","$$P(x_i | y_k) = \\frac{1}{\\sqrt{2\\pi \\sigma^2_{y_k}}} exp(- \\frac{x_i - \\mu_{y_k}}{2\\sigma^2_{y_k}}) $$\n","\n","where $\\sigma_{y_k} $ and $\\mu_{y_k}$ are estimated using maximum likelihood."]},{"cell_type":"markdown","metadata":{"id":"9fAc7EtyXgT_","colab_type":"text"},"source":["### Multinomial Naive Bayes\n","\n","It is used for multinomially distributed data and $P(x_i | y_k)$ is the probability of feature $i$ appearing in a sample belonging to class $y_k$.\n","\n","\n","$$P(x_i | y_k) = \\frac{number-of-examples-in-class-y_k-that-have-x_i}{number-of-examples-in-class-y_k}$$\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"EpyZVEkJXgUA","colab_type":"text"},"source":["## How to use scikit-learn"]},{"cell_type":"code","metadata":{"id":"Wzd9zlv_XgUB","colab_type":"code","colab":{}},"source":["# import the library\n","from sklearn.naive_bayes import GaussianNB, MultinomialNB\n","from sklearn.neighbors import KNeighborsClassifier\n","import numpy as np"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NPPjiRY8XgUE","colab_type":"text"},"source":["##### define the model\n","model = KNeighborsClassifier(n_neighbors=7, metric='minkowski') \n","\n","##### train the model\n","model.fit(X, y)\n","\n","##### predict the labels\n","predicted_labels = model.predict(X_test)\n","\n","##### compute the accuracy\n","accuracy = model.score(X_test, y_test)"]},{"cell_type":"markdown","metadata":{"id":"ijsH9caAXgUF","colab_type":"text"},"source":["# Execises"]},{"cell_type":"markdown","metadata":{"id":"GJF-5PlgXgUG","colab_type":"text"},"source":["### 1. Compute the accuracy of the multinomial naive bayes classifier on the MNIST subset."]},{"cell_type":"code","metadata":{"id":"lF9AnysVXgUG","colab_type":"code","colab":{}},"source":["# load data\n","train_images = np.load('data/train_images.npy') # load training images\n","train_labels = np.load('data/train_labels.npy') # load training labels\n","test_images = np.load('data/test_images.npy') # load testing images\n","test_labels = np.load('data/test_labels.npy') # load testing labels\n","\n","# write your code here\n","\n"],"execution_count":0,"outputs":[]}]}