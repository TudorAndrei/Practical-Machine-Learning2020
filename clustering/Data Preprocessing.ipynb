{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import unicodedata\n",
    "import regex as re\n",
    "import autocorrect\n",
    "from pandarallel import pandarallel\n",
    "import spacy\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "import random\n",
    "import numpy as np\n",
    "from multiprocessing import  Pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python -m spacy download en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = r'data/content_dataset.csv'\n",
    "data = pd.read_csv(path, encoding='utf-8')\n",
    "data_full = pd.read_csv(r'data/content_dataset_full.csv', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "spl = autocorrect.Speller(lang='en')\n",
    "\n",
    "    \n",
    "def remove_xa0(text):\n",
    "    # Remove the \\xa0 symbol from the text, this appears due to some encoding error\n",
    "    return unicodedata.normalize(\"NFKD\",  text)\n",
    "\n",
    "\n",
    "def remove_symbols(text):\n",
    "    # remove anything that's not alphanumeric and whitespace\n",
    "    pattern = '[^\\w\\s]|-|_'\n",
    "    return re.sub(pattern, \"\", text)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def stemmer(text):\n",
    "    stem = SnowballStemmer(language='english')\n",
    "    stems = []\n",
    "    for word in text.split():\n",
    "        stems.append(stem.stem(word))\n",
    "    \n",
    "    return \" \".join(stems)\n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "def preprocess(dataframe, sentences=1000, sample=True):\n",
    "    \n",
    "    if sample:\n",
    "        \n",
    "        df = dataframe.head(sentences).copy()\n",
    "    else:\n",
    "        df = dataframe.copy()\n",
    "    methods = {'remove \"\\\\xa0\"': remove_xa0,\n",
    "              'remove symbols': remove_symbols,\n",
    "              'stemming': stemmer}\n",
    "    \n",
    "    for method in methods.keys():\n",
    "#         print(f\"Performing method {method}\")\n",
    "        df.iloc[:, 0]= df.iloc[:, 0].apply(lambda x: methods[method](x))\n",
    "    \n",
    "#     pandarallel.initialize()\n",
    "    \n",
    "#     df.iloc[:, 0]= df.iloc[:, 0].parallel_apply(lemmatizer)\n",
    "#     df.iloc[:, 0]= df.iloc[:, 0].apply(lambda x: lemmatizer(x))\n",
    "#     for index in range(sentences):\n",
    "#         print(index)\n",
    "#         df.iloc[index, 0]= df.iloc[index, 0].apply(lambda x: lemmatizer(x), axis=1)\n",
    "    return df\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_df = preprocess(data_full, sample=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_df.to_csv(\"./data/processed_full.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def parallelize_dataframe(df, func, n_cores=8):\n",
    "#     df_split = np.array_split(df, n_cores)\n",
    "#     pool = Pool(n_cores)\n",
    "#     df = pd.concat(pool.map(func, df_split))\n",
    "#     pool.close()\n",
    "#     pool.join()\n",
    "#     return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# processed_df = parallelize_dataframe(data, preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# def spelling(text):\n",
    "#     # Correct spelling using the autocorret library\n",
    "   \n",
    "#     return spl(text) \n",
    "\n",
    "# def lemmatizer(text):\n",
    "#     # Split the corrected text in tokens, for each token save the lemma if the token is not a stopword and is longer than 2 characters\n",
    "#     words = []\n",
    "#     doc = nlp(spl(text), disable = ['ner'])\n",
    "# #     print(doc)\n",
    "#     for token in doc:\n",
    "#         if not token.is_punct and not token.is_stop and len(str(token)) > 2:\n",
    "# #                 print(f\"{token} -> {token.lemma_}\")\n",
    "#                 words.append(token.lemma_)\n",
    "#     del doc\n",
    "#     return \" \".join(words)\n",
    "# def lem(doc):\n",
    "#     words = []\n",
    "#     for token in doc:\n",
    "#         if not token.is_punct and not token.is_stop and len(str(token)) > 2:\n",
    "# #                 print(f\"{token} -> {token.lemma_}\")\n",
    "#                 words.append(token.lemma_)\n",
    "#     return \" \".join(words)\n",
    "\n",
    "# def lemmatize_df(df, target):\n",
    "    \n",
    "#     aux_df = pd.DataFrame(columns=['content'])\n",
    "    \n",
    "#     docs = nlp.pipe(data.content, disable=[\"tagger\", \"parser\", \"ner\"])\n",
    "#     for doc in docs:\n",
    "#         aux_df.append(pd.Series(lem(doc)), ignore_index=True)\n",
    "        \n",
    "#     return pd.concat([df, aux_df])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
